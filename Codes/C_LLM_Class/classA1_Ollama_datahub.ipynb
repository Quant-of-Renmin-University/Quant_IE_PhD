{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "387430FACB9D405E9A9C69D19BB1AFBA",
    "jupyter": {},
    "notebookId": "6937c9f47a0e19b0ec51112e",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!pip install modelscope gpustat\n",
    "#!modelscope download --model Fengshenbang/Erlangshen-RoBERTa-330M-Sentiment\n",
    "!pip install langchain-ollama ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "id": "FECAA581D33440F8B2C2409161076266",
    "jupyter": {},
    "notebookId": "6937c9f47a0e19b0ec51112e",
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-09 08:40:21--  https://gh.ddlc.top/https://github.com/ollama/ollama/releases/download/v0.13.2/ollama-linux-amd64.tgz\n",
      "Resolving gh.ddlc.top (gh.ddlc.top)... 104.19.89.51\n",
      "Connecting to gh.ddlc.top (gh.ddlc.top)|104.19.89.51|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1960719094 (1.8G) [application/octet-stream]\n",
      "Saving to: ‘ollama-linux-amd64.tgz.1’\n",
      "\n",
      "ollama-linux-amd64. 100%[===================>]   1.83G  14.4MB/s    in 2m 17s  \n",
      "\n",
      "utime(ollama-linux-amd64.tgz.1): No such file or directory\n",
      "2025-12-09 08:42:39 (13.7 MB/s) - ‘ollama-linux-amd64.tgz.1’ saved [1960719094/1960719094]\n",
      "\n",
      "tar (child): ollama-linux-amd64.tgz: Cannot open: No such file or directory\n",
      "tar (child): Error is not recoverable: exiting now\n",
      "tar: Child returned status 2\n",
      "tar: Error is not recoverable: exiting now\n",
      "time=2025-12-09T08:42:40.950Z level=INFO source=routes.go:1544 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES:GPU-d4cbbe92-05ec-a8ba-e2f1-b7992c8e7c93 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/mw/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false OLLAMA_VULKAN:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
      "time=2025-12-09T08:42:40.952Z level=INFO source=images.go:522 msg=\"total blobs: 10\"\n",
      "time=2025-12-09T08:42:40.952Z level=INFO source=images.go:529 msg=\"total unused blobs removed: 0\"\n",
      "time=2025-12-09T08:42:40.953Z level=INFO source=routes.go:1597 msg=\"Listening on 127.0.0.1:11434 (version 0.13.2)\"\n",
      "time=2025-12-09T08:42:40.954Z level=INFO source=runner.go:67 msg=\"discovering available GPUs...\"\n",
      "time=2025-12-09T08:42:40.954Z level=WARN source=runner.go:485 msg=\"user overrode visible devices\" CUDA_VISIBLE_DEVICES=GPU-d4cbbe92-05ec-a8ba-e2f1-b7992c8e7c93\n",
      "time=2025-12-09T08:42:40.954Z level=WARN source=runner.go:489 msg=\"if GPUs are not correctly discovered, unset and try again\"\n",
      "time=2025-12-09T08:42:40.958Z level=INFO source=server.go:392 msg=\"starting runner\" cmd=\"/usr/bin/ollama runner --ollama-engine --port 39199\"\n",
      "time=2025-12-09T08:42:41.299Z level=INFO source=server.go:392 msg=\"starting runner\" cmd=\"/usr/bin/ollama runner --ollama-engine --port 34786\"\n",
      "time=2025-12-09T08:42:41.474Z level=INFO source=runner.go:106 msg=\"experimental Vulkan support disabled.  To enable, set OLLAMA_VULKAN=1\"\n",
      "time=2025-12-09T08:42:41.475Z level=INFO source=server.go:392 msg=\"starting runner\" cmd=\"/usr/bin/ollama runner --ollama-engine --port 36726\"\n",
      "time=2025-12-09T08:42:41.808Z level=INFO source=types.go:42 msg=\"inference compute\" id=GPU-d4cbbe92-05ec-a8ba-e2f1-b7992c8e7c93 filter_id=\"\" library=CUDA compute=7.5 name=CUDA0 description=\"NVIDIA TITAN RTX\" libdirs=ollama,cuda_v12 driver=12.0 pci_id=0000:86:00.0 type=discrete total=\"24.0 GiB\" available=\"23.6 GiB\"\n",
      "^C\n",
      "Error: ollama server not responding - could not connect to ollama server, run 'ollama serve' to start it\n"
     ]
    }
   ],
   "source": [
    "!wget https://gh.ddlc.top/https://github.com/ollama/ollama/releases/download/v0.13.2/ollama-linux-amd64.tgz\n",
    "!sudo tar zxvf ollama-linux-amd64.tgz -C /usr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87367B622CBB4EE09D0BB085D18C61DE",
    "jupyter": {},
    "notebookId": "6937c9f47a0e19b0ec51112e",
    "runtime": {
     "execution_status": null,
     "is_visible": false,
     "status": "default"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### run codes in terminal  \n",
    "nohup ollama serve &  \n",
    "ollama pull gemma3:270m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "id": "EAB0B677D06C47DA8740D795FA841921",
    "jupyter": {},
    "notebookId": "6937c9f47a0e19b0ec51112e",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running batch inference...\n",
      "\n",
      "Prompt: Explain quantum computing in simple terms.\n",
      "Response: Imagine you have a box filled with LEGO bricks. You can only put one brick at a time. You can't put all the bricks in one go.\n",
      "\n",
      "Quantum computing is like having a super-powered box that can **store and manipulate information in a way that is impossible for regular computers.**\n",
      "\n",
      "Here's the breakdown:\n",
      "\n",
      "*   **Quantum mechanics is the study of the very small:** It's about the weird world of particles like electrons and photons.\n",
      "*   **Quantum computers use \"qubits\":** These are like dimmer switches. They can be in multiple states at the same time, allowing them to explore many possibilities simultaneously.\n",
      "*   **They can solve problems that are impossible for regular computers:** This is where quantum computing comes in. It could help us solve problems that are currently too hard for even the most powerful supercomputers.\n",
      "\n",
      "**Think of it like this:**\n",
      "\n",
      "*   **Regular computers:**  They do calculations based on bits (0s and 1s).\n",
      "*   **Quantum computers:** They can manipulate these bits in ways that are fundamentally different. They can explore many possibilities at once, potentially finding solutions to complex problems much faster.\n",
      "\n",
      "**In short:** Quantum computing is a revolutionary technology that uses the principles of quantum mechanics to solve problems that are currently intractable for classical computers. It's like building a super-powered box that can handle problems that are impossible for regular computers to solve.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: Write a haiku about artificial intelligence.\n",
      "Response: Code learns and grows,\n",
      "A silent, digital mind,\n",
      "Future's bright and bold.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: Translate this to French: 'The quick brown fox jumps over the lazy dog.'\n",
      "Response: The most common and natural translation of \"The quick brown fox jumps over the lazy dog\" in French is:\n",
      "\n",
      "**\"Le rapide renard brun saute par-dessus le chien paresseux.\"**\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: Summarize the theory of relativity in one paragraph.\n",
      "Response: Relativity theory posits that the fundamental laws of physics are the same for all observers, regardless of their motion or gravitational potential. This means that space and time are intertwined, and that the speed of light is the universal speed limit, which is absolute and constant for all observers.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: Tell me a short joke about programmers.\n",
      "Response: Why did the programmer get fired from their job? \n",
      "\n",
      "Because they couldn't figure out how to debug a complex algorithm!\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# Initialize the Ollama model (e.g., llama3.2, gemma2, phi3, etc.)\n",
    "llm = OllamaLLM(\n",
    "    model=\"gemma3:270m\",        # or \"gemma2\", \"mistral\", \"phi3\", etc.\n",
    "    temperature=0.7,\n",
    "    base_url=\"http://localhost:11434\",  # default, change if needed\n",
    ")\n",
    "\n",
    "# List of prompts you want to run in batch\n",
    "prompts = [\n",
    "    \"Explain quantum computing in simple terms.\",\n",
    "    \"Write a haiku about artificial intelligence.\",\n",
    "    \"Translate this to French: 'The quick brown fox jumps over the lazy dog.'\",\n",
    "    \"Summarize the theory of relativity in one paragraph.\",\n",
    "    \"Tell me a short joke about programmers.\",\n",
    "]\n",
    "\n",
    "print(\"Running batch inference...\\n\")\n",
    "\n",
    "# This invokes all prompts in parallel (batch mode)\n",
    "responses = llm.batch(prompts, config={\"max_concurrent\": 10})  # optional concurrency limit\n",
    "\n",
    "# Print results\n",
    "for prompt, response in zip(prompts, responses):\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {response.strip()}\\n\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "id": "F69C7791DF184AC8AE965B1A85C5DD95",
    "jupyter": {},
    "notebookId": "6937c9f47a0e19b0ec51112e",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running batch sentiment analysis on 7 reviews using gemma3:270m...\n",
      "\n",
      "Results:\n",
      "============================================================\n",
      " 1. [POSITIVE] (conf: 0.90) → I absolutely love this product! Best purchase ever....\n",
      " 2. [POSITIVE] (conf: 0.90) → It's okay, nothing special. Does the job....\n",
      " 3. [NEGATIVE] (conf: 0.95) → Terrible customer service and the item broke in half....\n",
      "     ↳ The customer service was extremely negative, with a broken item and a negative rating.\n",
      "\n",
      " 4. [POSITIVE] (conf: 0.80) → Super fast shipping and great quality!...\n",
      "     ↳ The customer is very satisfied with the fast shipping and quality of the product.\n",
      "\n",
      " 5. [POSITIVE] (conf: 0.80) → Not bad, but the color is different from the photo....\n",
      " 6. [NEGATIVE] (conf: 0.95) → Worst experience ever. Never buying again....\n",
      "     ↳ The customer expresses strong dissatisfaction with the experience, indicating a negative sentiment.\n",
      "\n",
      " 7. [NEGATIVE] (conf: 0.90) → Meh. Expected more for the price....\n",
      "     ↳ The customer was disappointed with the price and the product quality.\n",
      "\n",
      "Batch sentiment analysis completed!\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import json\n",
    "from typing import Literal\n",
    "\n",
    "# Choose one of these (make sure it's pulled with `ollama pull <name>`)\n",
    "MODEL = \"gemma3:270m\"          # ← super fast + accurate for sentiment\n",
    "# MODEL = \"llama3.2:3b\"      # also excellent\n",
    "# MODEL = \"gemma2:9b-it\"     # highest accuracy\n",
    "\n",
    "llm = OllamaLLM(\n",
    "    model=MODEL,\n",
    "    temperature=0.0,         # deterministic = consistent sentiment labels\n",
    "    format=\"json\",           # forces valid JSON output (works great with phi3/gemma2/llama3+)\n",
    ")\n",
    "\n",
    "# Zero-shot prompt that forces clean JSON output\n",
    "sentiment_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Analyze the sentiment of the following customer review.\n",
    "\n",
    "Return ONLY a valid JSON object with exactly these three keys:\n",
    "{{\"sentiment\": \"positive\" | \"negative\" | \"neutral\", \n",
    "  \"confidence\": a number between 0.0 and 1.0,\n",
    "  \"explanation\": short reason in 1 sentence}}\n",
    "\n",
    "Text: \"{text}\"\n",
    "\n",
    "JSON:\"\"\"\n",
    ")\n",
    "\n",
    "# Example dataset (replace with your own list, DataFrame column, etc.)\n",
    "reviews = [\n",
    "    \"I absolutely love this product! Best purchase ever.\",\n",
    "    \"It's okay, nothing special. Does the job.\",\n",
    "    \"Terrible customer service and the item broke in half.\",\n",
    "    \"Super fast shipping and great quality!\",\n",
    "    \"Not bad, but the color is different from the photo.\",\n",
    "    \"Worst experience ever. Never buying again.\",\n",
    "    \"Meh. Expected more for the price.\",\n",
    "]\n",
    "\n",
    "print(f\"Running batch sentiment analysis on {len(reviews)} reviews using {MODEL}...\\n\")\n",
    "\n",
    "# This runs all prompts in parallel (true batching)\n",
    "chain = sentiment_prompt | llm\n",
    "results = chain.batch([{\"text\": r} for r in reviews], config={\"max_concurrency\": 20})\n",
    "\n",
    "# Parse and display clean results\n",
    "print(\"Results:\\n\" + \"=\"*60)\n",
    "for i, raw_output in enumerate(results):\n",
    "    try:\n",
    "        # Clean common artifacts (some models add ```json or extra text)\n",
    "        json_str = raw_output.strip()\n",
    "        if json_str.startswith(\"```json\"):\n",
    "            json_str = json_str[7:-3]\n",
    "        elif json_str.startswith(\"```\"):\n",
    "            json_str = json_str[3:-3]\n",
    "        \n",
    "        data = json.loads(json_str)\n",
    "        \n",
    "        sentiment = data.get(\"sentiment\", \"unknown\").upper()\n",
    "        confidence = data.get(\"confidence\", 0.0)\n",
    "        explanation = data.get(\"explanation\", \"\")\n",
    "        \n",
    "        print(f\"{i+1:2d}. [{sentiment:8}] (conf: {confidence:.2f}) → {reviews[i][:60]}...\")\n",
    "        if explanation:\n",
    "            print(f\"     ↳ {explanation}\\n\")\n",
    "            \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"{i+1:2d}. [PARSE ERROR] Raw output: {raw_output}\\n\")\n",
    "\n",
    "print(\"Batch sentiment analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "1B29F0F4C4E7455A9F4CB5E1CC89C348",
    "jupyter": {},
    "notebookId": "6937c9f47a0e19b0ec51112e",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
