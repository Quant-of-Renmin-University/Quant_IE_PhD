{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cd97169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running batch inference...\n",
      "\n",
      "Prompt: Explain quantum computing in simple terms.\n",
      "Response: Imagine a light switch. It can be either on (representing a 1) or off (representing a 0). Quantum computing uses a light switch that can be both on and off at the same time, a process called superposition and entanglement.\n",
      "\n",
      "Here's how it works:\n",
      "\n",
      "* **Superposition:** Instead of being in a definite state (like a light switch), a quantum computer can be in multiple states simultaneously. Think of it like a coin spinning in the air – it's neither heads nor tails until it lands.\n",
      "* **Entanglement:** This is like having two of these coins linked together. If one is heads, the other instantly becomes tails, even if they're miles apart. This creates a powerful connection that allows the computer to explore a vast number of possibilities simultaneously.\n",
      "\n",
      "**Why is this powerful?**\n",
      "\n",
      "Because of superposition and entanglement, quantum computers can:\n",
      "\n",
      "* **Solve complex problems that are impossible for regular computers:** This includes problems like simulating molecules, understanding the universe, and developing new drugs.\n",
      "* **Make exponentially faster calculations:** Quantum computers can explore many possibilities at once, potentially solving problems much faster than classical computers.\n",
      "* **Store and process information in a fundamentally different way:** Quantum computers can store information in a way that is fundamentally different from what we experience in our everyday lives.\n",
      "\n",
      "**Think of it like this:**\n",
      "\n",
      "Imagine trying to figure out the location of a rare atom. You can't look at it directly, but you can use a very precise detector to try to find it. Quantum computers are like a super-powered magnifying glass that can see the atom in ways that are impossible for classical computers.\n",
      "\n",
      "**In short, quantum computing is a revolutionary approach to computing that uses the weirdness of quantum mechanics to perform calculations in a fundamentally different way.** It's still a very young field, but it has the potential to change the way we solve problems and understand the world around us.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: Write a haiku about artificial intelligence.\n",
      "Response: Code learns and grows,\n",
      "A silent, digital mind,\n",
      "Future's bright and new.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: Translate this to French: 'The quick brown fox jumps over the lazy dog.'\n",
      "Response: The translation is:\n",
      "\n",
      "**\"Le rapide renard brun saute par-dessus le chien paresseux.\"**\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: Summarize the theory of relativity in one paragraph.\n",
      "Response: Einstein's theory of relativity, a cornerstone of modern physics, describes how space and time are intertwined and can be warped by mass and energy. It postulates that the speed of light is a universal constant, meaning that nothing can travel faster than the speed of light. This leads to the concept of absolute time and space, where time and space are independent and are relative to the observer's motion.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: Tell me a short joke about programmers.\n",
      "Response: Why did the programmer get fired from his job? \n",
      "\n",
      "Because he kept trying to make the code *perfect*!\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# Initialize the Ollama model (e.g., llama3.2, gemma2, phi3, etc.)\n",
    "llm = OllamaLLM(\n",
    "    model=\"gemma3:270m\",        # or \"gemma2\", \"mistral\", \"phi3\", etc.\n",
    "    temperature=0.7,\n",
    "    base_url=\"http://localhost:11436\",  # default, change if needed\n",
    ")\n",
    "\n",
    "# List of prompts you want to run in batch\n",
    "prompts = [\n",
    "    \"Explain quantum computing in simple terms.\",\n",
    "    \"Write a haiku about artificial intelligence.\",\n",
    "    \"Translate this to French: 'The quick brown fox jumps over the lazy dog.'\",\n",
    "    \"Summarize the theory of relativity in one paragraph.\",\n",
    "    \"Tell me a short joke about programmers.\",\n",
    "]\n",
    "\n",
    "print(\"Running batch inference...\\n\")\n",
    "\n",
    "# This invokes all prompts in parallel (batch mode)\n",
    "responses = llm.batch(prompts, config={\"max_concurrent\": 10})  # optional concurrency limit\n",
    "\n",
    "# Print results\n",
    "for prompt, response in zip(prompts, responses):\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {response.strip()}\\n\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b4c98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running batch sentiment analysis on 7 reviews using gemma3:270m...\n",
      "\n",
      "Results:\n",
      "============================================================\n",
      " 1. [POSITIVE] (conf: 0.90) → I absolutely love this product! Best purchase ever....\n",
      "     ↳ The customer is highly satisfied with the product and expresses strong positive feelings.\n",
      "\n",
      " 2. [POSITIVE] (conf: 0.00) → It's okay, nothing special. Does the job....\n",
      " 3. [NEGATIVE] (conf: 0.95) → Terrible customer service and the item broke in half....\n",
      "     ↳ The customer service was extremely negative, with a broken item and a negative rating.\n",
      "\n",
      " 4. [POSITIVE] (conf: 0.90) → Super fast shipping and great quality!...\n",
      "     ↳ The customer is very satisfied with the fast shipping and quality of the product.\n",
      "\n",
      " 5. [POSITIVE] (conf: 0.80) → Not bad, but the color is different from the photo....\n",
      " 6. [NEGATIVE] (conf: 0.95) → Worst experience ever. Never buying again....\n",
      "     ↳ The customer expresses strong negative sentiment, indicating a negative experience.\n",
      "\n",
      " 7. [NEGATIVE] (conf: 0.90) → Meh. Expected more for the price....\n",
      "     ↳ The customer was disappointed with the product's quality and price.\n",
      "\n",
      "Batch sentiment analysis completed!\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import json\n",
    "from typing import Literal\n",
    "\n",
    "# Choose one of these (make sure it's pulled with `ollama pull <name>`)\n",
    "MODEL = \"gemma3:270m\"          # ← super fast + accurate for sentiment\n",
    "# MODEL = \"llama3.2:3b\"      # also excellent\n",
    "# MODEL = \"gemma2:9b-it\"     # highest accuracy\n",
    "\n",
    "llm = OllamaLLM(\n",
    "    model=MODEL,\n",
    "    temperature=0.0,         # deterministic = consistent sentiment labels\n",
    "    format=\"json\",           # forces valid JSON output (works great with phi3/gemma2/llama3+)\n",
    ")\n",
    "\n",
    "# Zero-shot prompt that forces clean JSON output\n",
    "sentiment_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Analyze the sentiment of the following customer review.\n",
    "\n",
    "Return ONLY a valid JSON object with exactly these three keys:\n",
    "{{\"sentiment\": \"positive\" | \"negative\" | \"neutral\", \n",
    "  \"confidence\": a number between 0.0 and 1.0,\n",
    "  \"explanation\": short reason in 1 sentence}}\n",
    "\n",
    "Text: \"{text}\"\n",
    "\n",
    "JSON:\"\"\"\n",
    ")\n",
    "\n",
    "# Example dataset (replace with your own list, DataFrame column, etc.)\n",
    "reviews = [\n",
    "    \"I absolutely love this product! Best purchase ever.\",\n",
    "    \"It's okay, nothing special. Does the job.\",\n",
    "    \"Terrible customer service and the item broke in half.\",\n",
    "    \"Super fast shipping and great quality!\",\n",
    "    \"Not bad, but the color is different from the photo.\",\n",
    "    \"Worst experience ever. Never buying again.\",\n",
    "    \"Meh. Expected more for the price.\",\n",
    "]\n",
    "\n",
    "print(f\"Running batch sentiment analysis on {len(reviews)} reviews using {MODEL}...\\n\")\n",
    "\n",
    "# This runs all prompts in parallel (true batching)\n",
    "chain = sentiment_prompt | llm\n",
    "results = chain.batch([{\"text\": r} for r in reviews], config={\"max_concurrency\": 20})\n",
    "\n",
    "# Parse and display clean results\n",
    "print(\"Results:\\n\" + \"=\"*60)\n",
    "for i, raw_output in enumerate(results):\n",
    "    try:\n",
    "        # Clean common artifacts (some models add ```json or extra text)\n",
    "        json_str = raw_output.strip()\n",
    "        if json_str.startswith(\"```json\"):\n",
    "            json_str = json_str[7:-3]\n",
    "        elif json_str.startswith(\"```\"):\n",
    "            json_str = json_str[3:-3]\n",
    "        \n",
    "        data = json.loads(json_str)\n",
    "        \n",
    "        sentiment = data.get(\"sentiment\", \"unknown\").upper()\n",
    "        confidence = data.get(\"confidence\", 0.0)\n",
    "        explanation = data.get(\"explanation\", \"\")\n",
    "        \n",
    "        print(f\"{i+1:2d}. [{sentiment:8}] (conf: {confidence:.2f}) → {reviews[i][:60]}...\")\n",
    "        if explanation:\n",
    "            print(f\"     ↳ {explanation}\\n\")\n",
    "            \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"{i+1:2d}. [PARSE ERROR] Raw output: {raw_output}\\n\")\n",
    "\n",
    "print(\"Batch sentiment analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8620b012",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
